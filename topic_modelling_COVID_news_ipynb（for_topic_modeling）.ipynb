{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_modelling_COVID_news.ipynb（for topic modeling）",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/workhardzy/K6312/blob/main/topic_modelling_COVID_news_ipynb%EF%BC%88for_topic_modeling%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMmqUeZ_0eRe"
      },
      "source": [
        "Loading the libraries needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR8ZY0mmuejK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4gPybm1ufPE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGsh_OfU0p3A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098d3b3e-5536-4ff4-cdc8-05f10ee1d3f0"
      },
      "source": [
        "#note you need to have it on your own google drive,for those who are sharing a drive, an approach is to add shortcut to your drive. \n",
        "#See https://stackoverflow.com/questions/54351852/accessing-shared-with-me-with-colab\n",
        "#put in your authorisation code that is linked to the google account you are linking to and press enter\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\",force_remount=True)\n",
        "!ls \"/content/gdrive/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtLJ8K4G00vr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e991373f-93d9-4962-9cbf-1e2f00750b92"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.15)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.17.0)\n",
            "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (50.3.2)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgdMVnU1z5-6"
      },
      "source": [
        "#open file\n",
        "import os\n",
        "#import display\n",
        "from IPython.display import display\n",
        "\n",
        "import pandas as pd\n",
        "import re, pickle, os\n",
        "import datetime \n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords, wordnet \n",
        "from collections import Counter \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora, models\n",
        "from gensim.corpora import MmCorpus\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import ast"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiaLZMdmnok6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d6a80f-4a4e-4a7d-a2e4-2dc852a7d589"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdxh0TTC1lBD"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARV3sFRW0ifV"
      },
      "source": [
        "def save_checkpoint(df,filepath):\n",
        "    df.to_pickle(filepath)\n",
        "    print('saved dataframe at {}'.format(filepath)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mWm1qQv1D88"
      },
      "source": [
        "#these are the functions to train the model\n",
        "\n",
        "def save_print_to_file(outfile, msg):\n",
        "    with open(outfile, 'w') as fp:\n",
        "        print(msg, file=fp)  \n",
        "\n",
        "def get_word_count(tweets_text, num_gram):\n",
        "    '''\n",
        "    Get common word counts\n",
        "    '''\n",
        "    n_grams = list(ngrams(tweets_text, num_gram))\n",
        "    \n",
        "    #stop_ngrams = ['local news']\n",
        "    #n_grams = [i for i in n_grams if i not in stop_ngrams]\n",
        "    \n",
        "    common_words = Counter(n_grams).most_common()\n",
        "    word_count = pd.DataFrame(data = common_words, \n",
        "                              columns=['word','frequency']) \n",
        "    # Convert list to string\n",
        "    word_count['word'] = word_count['word'].apply(' '.join)\n",
        "    # Plot word count graph\n",
        "    word_count.head(20).sort_values('frequency').plot.barh(\n",
        "            x='word', y='frequency', title='Word Frequency',figsize=(19,10))\n",
        "    plt.savefig(WORD_COUNT_FILE)\n",
        "    print ('Word count saved\\n')\n",
        "    plt.close('all')\n",
        "    \n",
        "    return word_count\n",
        "\n",
        "def word_grams(words, min=1, max=2):\n",
        "    '''\n",
        "    Build ngrams word list\n",
        "    '''\n",
        "    word_list = []\n",
        "    for n in range(min, max):\n",
        "        for ngram in ngrams(words, n):\n",
        "            #print(ngram)\n",
        "            word_list.append(' '.join(str(i) for i in ngram)) \n",
        "    return word_list\n",
        "\n",
        "def train_lda_model(token_tweets):\n",
        "    print('Start LDA model training ...\\n')    \n",
        "    # Build dictionary\n",
        "    print('building dictionary')\n",
        "    tweets_dict = corpora.Dictionary(token_tweets)\n",
        "    # Remove words that occur less than 10 documents, # or more than 50% of the doc\n",
        "    tweets_dict.filter_extremes(no_below=10, no_above=0.5)                       ########调\n",
        "    # Transform doc to a vectorized form by computing frequency of each word\n",
        "    bow_corpus = [tweets_dict.doc2bow(doc) for doc in token_tweets]\n",
        "    # Save corpus and dictionary to file\n",
        "    MmCorpus.serialize(CORPUS_FILE, bow_corpus)\n",
        "    tweets_dict.save(DICT_FILE)\n",
        "    print('saved corpus and dictionary to file')\n",
        "    \n",
        "    # Create tf-idf model and then apply transformation to the entire corpus\n",
        "    print('create tf-idf model')\n",
        "    tfidf = models.TfidfModel(bow_corpus)                                       ########调 (optional)\n",
        "    tfidf_corpus = tfidf[bow_corpus]                                            ########调 (optional)\n",
        "    \n",
        "    print('training lda')\n",
        "    # Train LDA model #this is the time bottleneck\n",
        "    lda_model = models.ldamodel.LdaModel(corpus=tfidf_corpus,                   ########调 (optional)\n",
        "                                         num_topics=NUM_TOPICS, \n",
        "                                         id2word=tweets_dict, \n",
        "                                         passes=NUM_PASSES, \n",
        "                                         alpha=ALPHA, \n",
        "                                         eta=ETA,\n",
        "                                         random_state=42)\n",
        "    # Save LDA model to file\n",
        "    lda_model.save(LDA_MODEL_FILE)\n",
        "    print ('LDA model saved\\n')\n",
        "      \n",
        "    # Save all generated topics to a file\n",
        "    msg = ''\n",
        "    for idx, topic in lda_model.print_topics(-1):\n",
        "        msg += 'Topic: {} \\nWords: {}\\n'.format(idx, topic)    \n",
        "    save_print_to_file(LDA_TOPICS_FILE, msg)\n",
        "    \n",
        "    # Evaluate LDA model performance\n",
        "    eval_lda (lda_model, tfidf_corpus, tweets_dict, token_tweets)    \n",
        "    # Visualize topics\n",
        "    vis_topics(lda_model, tfidf_corpus, tweets_dict)\n",
        "        \n",
        "    return lda_model\n",
        "\n",
        "def eval_lda (lda_model, corpus, dict, token_text):\n",
        "    \n",
        "    # Compute Perplexity: a measure of how good the model is. lower the better.\n",
        "    print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  \n",
        "    \n",
        "    # Compute Coherence Score\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=token_text, \n",
        "                                         dictionary=dict, coherence='c_v')   \n",
        "    print('\\nCoherence: ', coherence_model_lda.get_coherence())\n",
        "\n",
        "def vis_topics(lda_model, corpus, dict):\n",
        "    '''\n",
        "    Plot generated topics on an interactive graph\n",
        "    '''\n",
        "    lda_data =  pyLDAvis.gensim.prepare(lda_model, corpus, dict, mds='mmds')\n",
        "    pyLDAvis.display(lda_data)\n",
        "    pyLDAvis.save_html(lda_data, TOPIC_VIS_FILE)\n",
        "    print ('Topic visual saved\\n')\n",
        "\n",
        "def wordcloud(word_count_df):\n",
        "    '''\n",
        "    Create word cloud image\n",
        "    '''\n",
        "    # Convert DataFrame to Map so that word cloud can be generated from freq\n",
        "    word_count_dict = {}\n",
        "    for w, f in word_count_df.values:\n",
        "        word_count_dict[w] = f\n",
        "    # Generate word cloud \n",
        "    wordcloud = WordCloud(max_words=300, width=1400, height=900, \n",
        "                          random_state=12, contour_width=3, \n",
        "                          contour_color='firebrick')\n",
        "    wordcloud.generate_from_frequencies(word_count_dict)\n",
        "    plt.figure(figsize=(10,10), facecolor='k')\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    # Save the word cloud image\n",
        "    wordcloud.to_file(WORDCLOUD_FILE) \n",
        "    print ('Word cloud saved\\n')\n",
        "    plt.close('all')\n",
        "    \n",
        "    return wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o70r2kK1Rm_"
      },
      "source": [
        "'''\n",
        "def clean_topics(df):\n",
        "    print('CLEANING...')\n",
        "    cleaned_tweets_df = preprocess_tweets(df,'tweet_text') ########调\n",
        "    cleaned_tweets_df.to_pickle('/content/gdrive/My Drive/group/data/preprocessed_news_headlines.h5') ########调\n",
        "    return cleaned_tweets_df\n",
        "'''    \n",
        "def generate_ngrams(cleaned_tweets_df):\n",
        "    # Convert series to list for word count\n",
        "    print('creating tweets_text corpus')\n",
        "    tweets_text = [word for one_tweet in cleaned_tweets_df['token'] for word in one_tweet]\n",
        "    \n",
        "    print('getting ngram word count')\n",
        "    # Get common ngrams word count\n",
        "    word_count_df = get_word_count(tweets_text, num_gram=NUM_GRAMS)\n",
        "    \n",
        "    # Generate word cloud\n",
        "    tweets_wordcloud = wordcloud(word_count_df)  \n",
        "\n",
        "    # Generate ngram tokens\n",
        "    cleaned_tweets_df['ngram_token'] = [word_grams(x, NUM_GRAMS, NUM_GRAMS+1) for x in cleaned_tweets_df['token']]\n",
        "    \n",
        "    \n",
        "    return cleaned_tweets_df\n",
        "\n",
        "    \n",
        "def trainingLDA(ngrams):\n",
        "    print('TRAINING...')\n",
        "    # Train LDA model and visualize generated topics\n",
        "    lda_model = train_lda_model(ngrams)\n",
        "\n",
        "    print('DONE!')\n",
        "    \n",
        "    return lda_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV21WYpM2Cfa"
      },
      "source": [
        "Here we start training the model and the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOXj4gs11SSg"
      },
      "source": [
        "#path variables\n",
        "mth = 9   #to change the month                                                  ########调\n",
        "FIGURE_PATH = r'gdrive/My Drive/group/figures/'                          ########调 #need to create the folder\n",
        "DATA_PATH = r'gdrive/My Drive/group/data/'                               ########调 #need to create the folder\n",
        "MODEL_PATH = r'gdrive/My Drive/group/models/'                            ########调 #need to create the folder\n",
        "WORDCLOUD_FILE = FIGURE_PATH + 'wordcloud{}.png'.format(mth)\n",
        "WORD_COUNT_FILE = FIGURE_PATH + 'commond_words_freq{}.png'.format(mth)\n",
        "TOPIC_VIS_FILE = FIGURE_PATH + 'lda{}.html'.format(mth)\n",
        "ORIG_TWEET_FILE = DATA_PATH + 'all_tweets'                                      ########换名字 \n",
        "CLEANED_TWEET_FILE = DATA_PATH + 'tweets_cleaned_df'                            ########换名字 \n",
        "CORPUS_FILE = MODEL_PATH + 'clean_tweets_corpu{}s.mm'.format(mth)                             ########换名字 \n",
        "DICT_FILE = MODEL_PATH + 'clean_tweet{}s.dict'.format(mth)                                    ########换名字 \n",
        "LDA_MODEL_FILE = MODEL_PATH + 'tweets_lda{}.model'.format(mth)                                ########换名字 \n",
        "LDA_TOPICS_FILE = MODEL_PATH + 'tweets_lda_topics{}.txt'.format(mth)                          ########换名字 \n",
        "\n",
        "\n",
        "#tunable parameters\n",
        "NUM_GRAMS = 2 #ngrams                                                                                                                    ########调 (optional)\n",
        "NUM_TOPICS = 5 #guesstimate  #要用分析                                                                                                   ########调 \n",
        "NUM_PASSES = 50 #Number of training passes/iterations over all tweets   #越高越久，但应该会准                                              ########调 (optional)         \n",
        "ALPHA = 'auto'  #the lower alpha is, the more likely that a tweet may contain mixture of just a few of the topics 1/auto is normal\n",
        "ETA = 'auto' # Word-Topic Density.The lower eta is, the more likely that a topic may contain a mixture of just a few of the words\n",
        "\n",
        "#system variables\n",
        "WORDCLOUD_FILE = FIGURE_PATH + 'wordcloud{}.png'.format(mth)                                  ########换名字 \n",
        "WORD_COUNT_FILE = FIGURE_PATH + 'common_words_freq{}.png'.format(mth)                         ########换名字   \n",
        "TOPIC_VIS_FILE = FIGURE_PATH + 'lda{}.html'.format(mth)                                       ########换名字 \n",
        "CORPUS_FILE = MODEL_PATH + 'clean_tweets_corpus{}.mm'.format(mth)                             ########换名字 \n",
        "DICT_FILE = MODEL_PATH + 'clean_tweets{}.dict'.format(mth)                                    ########换名字 \n",
        "LDA_MODEL_FILE = MODEL_PATH + 'tweets_lda{}.model'.format(mth)                                ########换名字 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxNGQd1U5aS3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "98c93ce6-f910-4958-8a44-69997bea98b2"
      },
      "source": [
        "df = pd.read_pickle('/content/gdrive/My Drive/group/data/preprocessed_news_headlines{}.h5'.format(mth)) ########调\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>topic_area</th>\n",
              "      <th>token</th>\n",
              "      <th>ngram_token</th>\n",
              "      <th>month</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>273937</th>\n",
              "      <td>Samsung eyes new opportunities as pandemic ignites demand for home appliances</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[samsung, eye, new, opportunity, pandemic, ignites, demand, home, appliance]</td>\n",
              "      <td>[samsung eye, eye new, new opportunity, opportunity pandemic, pandemic ignites, ignites demand, demand home, home appliance]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273938</th>\n",
              "      <td>Horse racing: Tiz the Law arrives, Art Collector exits at Churchill Downs</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[horse, race, tiz, law, arrives, art, collector, exit, churchill]</td>\n",
              "      <td>[horse race, race tiz, tiz law, law arrives, arrives art, art collector, collector exit, exit churchill]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273939</th>\n",
              "      <td>US Open 2020: Osaka passes Doi test and Pliskova advances but Gauff falls</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[u, open, osaka, pass, doi, test, pliskova, advance, gauff, fall]</td>\n",
              "      <td>[u open, open osaka, osaka pass, pass doi, doi test, test pliskova, pliskova advance, advance gauff, gauff fall]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273940</th>\n",
              "      <td>The future of sports TV coverage? Eurosport unveils mixed-reality 'Cube' studio for US Open</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[future, sport, tv, coverage, eurosport, unveils, mixed, reality, cube, studio, u, open]</td>\n",
              "      <td>[future sport, sport tv, tv coverage, coverage eurosport, eurosport unveils, unveils mixed, mixed reality, reality cube, cube studio, studio u, u open]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273941</th>\n",
              "      <td>U.S. employment projected to increase six million from 2019 to 2029: Labor Department</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[u, employment, project, increase, six, million, labor, department]</td>\n",
              "      <td>[u employment, employment project, project increase, increase six, six million, million labor, labor department]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290474</th>\n",
              "      <td>Trump signs new, expanded executive order to lower U.S. drug prices</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[trump, sign, new, expand, executive, order, lower, u, drug, price]</td>\n",
              "      <td>[trump sign, sign new, new expand, expand executive, executive order, order lower, lower u, u drug, drug price]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290475</th>\n",
              "      <td>Nuggets force Game 7 and D'Antoni leaves HOU</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[nugget, force, game, antoni, leaf, hou]</td>\n",
              "      <td>[nugget force, force game, game antoni, antoni leaf, leaf hou]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290476</th>\n",
              "      <td>Health Official Out To Manipulate CDC Reports Has Deep Russian Ties</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[health, official, manipulate, cdc, report, deep, russian, tie]</td>\n",
              "      <td>[health official, official manipulate, manipulate cdc, cdc report, report deep, deep russian, russian tie]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290477</th>\n",
              "      <td>Global Markets: Asian shares buoyed by coronavirus vaccine hopes</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[global, market, asian, share, buoyed, coronavirus, vaccine, hope]</td>\n",
              "      <td>[global market, market asian, asian share, share buoyed, buoyed coronavirus, coronavirus vaccine, vaccine hope]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290478</th>\n",
              "      <td>Japan’s Suga Wins Ruling Party Race to Replace Premier Abe</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[japan, suga, win, ruling, party, race, replace, premier, abe]</td>\n",
              "      <td>[japan suga, suga win, win ruling, ruling party, party race, race replace, replace premier, premier abe]</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16301 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              title  ... month\n",
              "273937  Samsung eyes new opportunities as pandemic ignites demand for home appliances                ...  9   \n",
              "273938  Horse racing: Tiz the Law arrives, Art Collector exits at Churchill Downs                    ...  9   \n",
              "273939  US Open 2020: Osaka passes Doi test and Pliskova advances but Gauff falls                    ...  9   \n",
              "273940  The future of sports TV coverage? Eurosport unveils mixed-reality 'Cube' studio for US Open  ...  9   \n",
              "273941  U.S. employment projected to increase six million from 2019 to 2029: Labor Department        ...  9   \n",
              "...                                                                                       ...        ... ..   \n",
              "290474  Trump signs new, expanded executive order to lower U.S. drug prices                          ...  9   \n",
              "290475  Nuggets force Game 7 and D'Antoni leaves HOU                                                 ...  9   \n",
              "290476  Health Official Out To Manipulate CDC Reports Has Deep Russian Ties                          ...  9   \n",
              "290477  Global Markets: Asian shares buoyed by coronavirus vaccine hopes                             ...  9   \n",
              "290478  Japan’s Suga Wins Ruling Party Race to Replace Premier Abe                                   ...  9   \n",
              "\n",
              "[16301 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpvSSR2L2kSJ"
      },
      "source": [
        "#training the model\n",
        "\n",
        "can skip if not needed to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_yKswmBhMk3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "42848727-6fdd-45e9-cbc5-9dc454d8dd5b"
      },
      "source": [
        "#This is the part where we suppress warnings as it is occupying too much memory\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokens = df['ngram_token']\n",
        "\n",
        "%time LDAmodel = trainingLDA(tokens) #training done, #this portion takes a lot of time \n",
        "\n",
        "'''\n",
        "CPU times: user 54min 36s, sys: 15.8 s, total: 54min 52s\n",
        "Wall time: 55min 8s\n",
        "\n",
        "Perplexity, the lower the better -93.7418325786182\n",
        "\n",
        "Coherence, the higher the better 0.6607926560682633\n",
        "'''\n",
        "'''\n",
        "Perplexity:  -5.348532548095669\n",
        "\n",
        "Coherence:  0.5406701187135824\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING...\n",
            "Start LDA model training ...\n",
            "\n",
            "building dictionary\n",
            "saved corpus and dictionary to file\n",
            "create tf-idf model\n",
            "training lda\n",
            "LDA model saved\n",
            "\n",
            "\n",
            "Perplexity:  -6.608657366805812\n",
            "\n",
            "Coherence:  0.757562918211186\n",
            "Topic visual saved\n",
            "\n",
            "DONE!\n",
            "CPU times: user 1min 11s, sys: 124 ms, total: 1min 11s\n",
            "Wall time: 1min 13s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nPerplexity:  -5.348532548095669\\n\\nCoherence:  0.5406701187135824\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AuJT-HNo1n0"
      },
      "source": [
        "#to analyse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK0MdysyxgdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d991f42-9a41-494d-ef2b-eab1da842ad7"
      },
      "source": [
        "#showing the topics and the keywords\n",
        "LDA_MODEL_FILE = MODEL_PATH + 'tweets_lda{}.model'.format(mth) \n",
        "ldaModel = models.ldamodel.LdaModel.load(LDA_MODEL_FILE)\n",
        "for j in ldaModel.print_topics(-1):\n",
        "    print(j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '0.033*\"donald trump\" + 0.033*\"covid case\" + 0.029*\"prime minister\" + 0.028*\"news guardian\" + 0.025*\"stock market\" + 0.024*\"researchandmarkets com\" + 0.024*\"win u\" + 0.021*\"amid pandemic\" + 0.020*\"season opener\" + 0.018*\"face mask\"')\n",
            "(1, '0.049*\"new york\" + 0.024*\"bob woodward\" + 0.023*\"south korea\" + 0.022*\"film festival\" + 0.022*\"u k\" + 0.021*\"coronavirus pandemic\" + 0.018*\"first time\" + 0.016*\"york city\" + 0.016*\"u election\" + 0.015*\"asia today\"')\n",
            "(2, '0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"')\n",
            "(3, '0.075*\"covid vaccine\" + 0.046*\"market growth\" + 0.043*\"growth technavio\" + 0.043*\"boost market\" + 0.041*\"vaccine trial\" + 0.035*\"indoor rally\" + 0.023*\"premier league\" + 0.021*\"roadmap recovery\" + 0.021*\"new zealand\" + 0.020*\"boris johnson\"')\n",
            "(4, '0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz4ka2Z1g2Av"
      },
      "source": [
        "#This is the part where we suppress warnings as it is occupying too much memory\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "#creating a dataframe for creating topic \n",
        "#load model\n",
        "#load dataframe\n",
        "\n",
        "def create_topic_df(df):\n",
        "    CORPUS_FILE = MODEL_PATH + 'clean_tweets_corpus{}.mm'.format(mth)                      ########换名字 \n",
        "    DICT_FILE = MODEL_PATH + 'clean_tweets{}.dict'.format(mth)                             ########换名字 \n",
        "    LDA_MODEL_FILE = MODEL_PATH + 'tweets_lda{}.model'.format(mth)                         ########换名字 \n",
        "\n",
        "    print(LDA_MODEL_FILE)\n",
        "    ldaModel = models.ldamodel.LdaModel.load(LDA_MODEL_FILE)\n",
        "    #load corpus\n",
        "    print(CORPUS_FILE)\n",
        "    corpus = MmCorpus(CORPUS_FILE)\n",
        "    \n",
        "    d_lookup = pd.DataFrame(ldaModel.print_topics(-1))\n",
        "    \n",
        "    topic_LDA_df = pd.DataFrame()\n",
        "    \n",
        "    for j in tqdm(range(len(corpus))):   \n",
        "        tldf = pd.DataFrame(ldaModel[corpus[j]]).transpose().drop(0)\n",
        "        a = tldf\n",
        "        tldf['max_pred_value']= a.max(axis=1)\n",
        "        tldf['pred_topic'] = a.idxmax(axis=1)\n",
        "        tldf['topic_label'] = d_lookup[d_lookup[0]==tldf['pred_topic'].iloc[0]][1].iloc[0]\n",
        "        topic_LDA_df = topic_LDA_df.append(tldf)\n",
        "\n",
        "    topic_LDA_df.reset_index(drop=True,inplace=True)\n",
        "\n",
        "    topic_LDA_df = topic_LDA_df.add_prefix('topic_')\n",
        "\n",
        "    return topic_LDA_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qJnaBD2pnYT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "e3d21740-3280-4569-bb6f-f4be99c50ba6"
      },
      "source": [
        "CORPUS_FILE = MODEL_PATH + 'clean_tweets_corpus{}.mm'.format(mth)                      ########换名字 \n",
        "DICT_FILE = MODEL_PATH + 'clean_tweets{}.dict'.format(mth)                             ########换名字 \n",
        "LDA_MODEL_FILE = MODEL_PATH + 'tweets_lda{}.model'.format(mth)                         ########换名字 \n",
        "\n",
        "print(LDA_MODEL_FILE)\n",
        "ldaModel = models.ldamodel.LdaModel.load(LDA_MODEL_FILE)\n",
        "#load corpus\n",
        "print(CORPUS_FILE)\n",
        "corpus = MmCorpus(CORPUS_FILE)\n",
        "\n",
        "d_lookup = pd.DataFrame(ldaModel.print_topics(-1))\n",
        "\n",
        "topic_LDA_df = pd.DataFrame()\n",
        "\n",
        "for j in tqdm(range(10)):   \n",
        "    tldf = pd.DataFrame(ldaModel[corpus[j]]).transpose().drop(0)\n",
        "    a = tldf\n",
        "    tldf['max_pred_value']= a.max(axis=1)\n",
        "    tldf['pred_topic'] = a.idxmax(axis=1)\n",
        "    tldf['topic_label'] = d_lookup[d_lookup[0]==tldf['pred_topic'].iloc[0]][1].iloc[0]\n",
        "    topic_LDA_df = topic_LDA_df.append(tldf)\n",
        "\n",
        "topic_LDA_df.reset_index(drop=True,inplace=True)\n",
        "\n",
        "topic_LDA_df = topic_LDA_df.add_prefix('topic_')\n",
        "topic_LDA_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 140.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/group/models/tweets_lda9.model\n",
            "gdrive/My Drive/group/models/clean_tweets_corpus9.mm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_max_pred_value</th>\n",
              "      <th>topic_pred_topic</th>\n",
              "      <th>topic_topic_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.153535</td>\n",
              "      <td>0.138480</td>\n",
              "      <td>0.162979</td>\n",
              "      <td>0.141370</td>\n",
              "      <td>0.403637</td>\n",
              "      <td>0.403637</td>\n",
              "      <td>4</td>\n",
              "      <td>0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.153535</td>\n",
              "      <td>0.138480</td>\n",
              "      <td>0.162979</td>\n",
              "      <td>0.141370</td>\n",
              "      <td>0.403637</td>\n",
              "      <td>0.403637</td>\n",
              "      <td>4</td>\n",
              "      <td>0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.153535</td>\n",
              "      <td>0.138480</td>\n",
              "      <td>0.162979</td>\n",
              "      <td>0.399626</td>\n",
              "      <td>0.145381</td>\n",
              "      <td>0.399626</td>\n",
              "      <td>3</td>\n",
              "      <td>0.075*\"covid vaccine\" + 0.046*\"market growth\" + 0.043*\"growth technavio\" + 0.043*\"boost market\" + 0.041*\"vaccine trial\" + 0.035*\"indoor rally\" + 0.023*\"premier league\" + 0.021*\"roadmap recovery\" + 0.021*\"new zealand\" + 0.020*\"boris johnson\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.122022</td>\n",
              "      <td>0.110057</td>\n",
              "      <td>0.540026</td>\n",
              "      <td>0.112354</td>\n",
              "      <td>0.115541</td>\n",
              "      <td>0.540026</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    topic_0  ...                                                                                                                                                                                                                                         topic_topic_label\n",
              "0  0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "1  0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "2  0.153535  ...  0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"                                       \n",
              "3  0.153535  ...  0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"                                       \n",
              "4  0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "5  0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "6  0.153535  ...  0.075*\"covid vaccine\" + 0.046*\"market growth\" + 0.043*\"growth technavio\" + 0.043*\"boost market\" + 0.041*\"vaccine trial\" + 0.035*\"indoor rally\" + 0.023*\"premier league\" + 0.021*\"roadmap recovery\" + 0.021*\"new zealand\" + 0.020*\"boris johnson\"        \n",
              "7  0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "8  0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "9  0.122022  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_MZ8pqug5Sa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "051fe6c8-a603-4904-aa88-398649fc46af"
      },
      "source": [
        "#execution\n",
        "dd = create_topic_df(df) #this takes a really long time. 1:19:37. 1 Looking to optimise this for future work\n",
        "display(dd)\n",
        "save_checkpoint(dd,'/content/gdrive/My Drive/group/data/topic_modelled_news_headlines{}.h5'.format(mth)) ########调\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 16/16301 [00:00<01:44, 156.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "gdrive/My Drive/group/models/tweets_lda9.model\n",
            "gdrive/My Drive/group/models/clean_tweets_corpus9.mm\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16301/16301 [01:43<00:00, 158.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_max_pred_value</th>\n",
              "      <th>topic_pred_topic</th>\n",
              "      <th>topic_topic_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.153535</td>\n",
              "      <td>0.138480</td>\n",
              "      <td>0.162979</td>\n",
              "      <td>0.141370</td>\n",
              "      <td>0.403637</td>\n",
              "      <td>0.403637</td>\n",
              "      <td>4</td>\n",
              "      <td>0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.153535</td>\n",
              "      <td>0.138480</td>\n",
              "      <td>0.162979</td>\n",
              "      <td>0.141370</td>\n",
              "      <td>0.403637</td>\n",
              "      <td>0.403637</td>\n",
              "      <td>4</td>\n",
              "      <td>0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16296</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16297</th>\n",
              "      <td>0.206992</td>\n",
              "      <td>0.186695</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>0.190591</td>\n",
              "      <td>0.195999</td>\n",
              "      <td>0.219724</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16298</th>\n",
              "      <td>0.122022</td>\n",
              "      <td>0.110057</td>\n",
              "      <td>0.334777</td>\n",
              "      <td>0.317603</td>\n",
              "      <td>0.115541</td>\n",
              "      <td>0.334777</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16299</th>\n",
              "      <td>0.101242</td>\n",
              "      <td>0.091314</td>\n",
              "      <td>0.448062</td>\n",
              "      <td>0.263516</td>\n",
              "      <td>0.095865</td>\n",
              "      <td>0.448062</td>\n",
              "      <td>2</td>\n",
              "      <td>0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16300</th>\n",
              "      <td>0.122022</td>\n",
              "      <td>0.110057</td>\n",
              "      <td>0.129527</td>\n",
              "      <td>0.522853</td>\n",
              "      <td>0.115541</td>\n",
              "      <td>0.522853</td>\n",
              "      <td>3</td>\n",
              "      <td>0.075*\"covid vaccine\" + 0.046*\"market growth\" + 0.043*\"growth technavio\" + 0.043*\"boost market\" + 0.041*\"vaccine trial\" + 0.035*\"indoor rally\" + 0.023*\"premier league\" + 0.021*\"roadmap recovery\" + 0.021*\"new zealand\" + 0.020*\"boris johnson\"</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16301 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        topic_0  ...                                                                                                                                                                                                                                         topic_topic_label\n",
              "0      0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "1      0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "2      0.153535  ...  0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"                                       \n",
              "3      0.153535  ...  0.098*\"u open\" + 0.041*\"covid test\" + 0.032*\"coronavirus case\" + 0.030*\"test positive\" + 0.026*\"first half\" + 0.025*\"grand slam\" + 0.022*\"rule six\" + 0.019*\"oil price\" + 0.019*\"hong kong\" + 0.018*\"opinion cnn\"                                       \n",
              "4      0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "...         ...  ...                                                                                                                                                                                                                                                       ...\n",
              "16296  0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "16297  0.206992  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "16298  0.122022  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "16299  0.101242  ...  0.055*\"coronavirus vaccine\" + 0.034*\"wall street\" + 0.022*\"report marketscreener\" + 0.022*\"labor day\" + 0.019*\"covid pandemic\" + 0.019*\"global market\" + 0.019*\"update marketscreener\" + 0.018*\"recovery covid\" + 0.017*\"white house\" + 0.017*\"japan pm\"\n",
              "16300  0.122022  ...  0.075*\"covid vaccine\" + 0.046*\"market growth\" + 0.043*\"growth technavio\" + 0.043*\"boost market\" + 0.041*\"vaccine trial\" + 0.035*\"indoor rally\" + 0.023*\"premier league\" + 0.021*\"roadmap recovery\" + 0.021*\"new zealand\" + 0.020*\"boris johnson\"        \n",
              "\n",
              "[16301 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "saved dataframe at /content/gdrive/My Drive/group/data/topic_modelled_news_headlines9.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOFUaIEaeK6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b339f5f5-7c9c-4445-e4db-ada971d5d190"
      },
      "source": [
        "pred = pd.read_pickle('/content/gdrive/My Drive/group/data/topic_modelled_news_headlines{}.h5'.format(mth)) ########调\n",
        "df2 = df.join(pred)\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>topic_area</th>\n",
              "      <th>token</th>\n",
              "      <th>ngram_token</th>\n",
              "      <th>month</th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_max_pred_value</th>\n",
              "      <th>topic_pred_topic</th>\n",
              "      <th>topic_topic_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>273937</th>\n",
              "      <td>Samsung eyes new opportunities as pandemic ignites demand for home appliances</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[samsung, eye, new, opportunity, pandemic, ignites, demand, home, appliance]</td>\n",
              "      <td>[samsung eye, eye new, new opportunity, opportunity pandemic, pandemic ignites, ignites demand, demand home, home appliance]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273938</th>\n",
              "      <td>Horse racing: Tiz the Law arrives, Art Collector exits at Churchill Downs</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[horse, race, tiz, law, arrives, art, collector, exit, churchill]</td>\n",
              "      <td>[horse race, race tiz, tiz law, law arrives, arrives art, art collector, collector exit, exit churchill]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273939</th>\n",
              "      <td>US Open 2020: Osaka passes Doi test and Pliskova advances but Gauff falls</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[u, open, osaka, pass, doi, test, pliskova, advance, gauff, fall]</td>\n",
              "      <td>[u open, open osaka, osaka pass, pass doi, doi test, test pliskova, pliskova advance, advance gauff, gauff fall]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273940</th>\n",
              "      <td>The future of sports TV coverage? Eurosport unveils mixed-reality 'Cube' studio for US Open</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[future, sport, tv, coverage, eurosport, unveils, mixed, reality, cube, studio, u, open]</td>\n",
              "      <td>[future sport, sport tv, tv coverage, coverage eurosport, eurosport unveils, unveils mixed, mixed reality, reality cube, cube studio, studio u, u open]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273941</th>\n",
              "      <td>U.S. employment projected to increase six million from 2019 to 2029: Labor Department</td>\n",
              "      <td>2020-09-01</td>\n",
              "      <td>business</td>\n",
              "      <td>[u, employment, project, increase, six, million, labor, department]</td>\n",
              "      <td>[u employment, employment project, project increase, increase six, six million, million labor, labor department]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290474</th>\n",
              "      <td>Trump signs new, expanded executive order to lower U.S. drug prices</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[trump, sign, new, expand, executive, order, lower, u, drug, price]</td>\n",
              "      <td>[trump sign, sign new, new expand, expand executive, executive order, order lower, lower u, u drug, drug price]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290475</th>\n",
              "      <td>Nuggets force Game 7 and D'Antoni leaves HOU</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[nugget, force, game, antoni, leaf, hou]</td>\n",
              "      <td>[nugget force, force game, game antoni, antoni leaf, leaf hou]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290476</th>\n",
              "      <td>Health Official Out To Manipulate CDC Reports Has Deep Russian Ties</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[health, official, manipulate, cdc, report, deep, russian, tie]</td>\n",
              "      <td>[health official, official manipulate, manipulate cdc, cdc report, report deep, deep russian, russian tie]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290477</th>\n",
              "      <td>Global Markets: Asian shares buoyed by coronavirus vaccine hopes</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[global, market, asian, share, buoyed, coronavirus, vaccine, hope]</td>\n",
              "      <td>[global market, market asian, asian share, share buoyed, buoyed coronavirus, coronavirus vaccine, vaccine hope]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290478</th>\n",
              "      <td>Japan’s Suga Wins Ruling Party Race to Replace Premier Abe</td>\n",
              "      <td>2020-09-14</td>\n",
              "      <td>business</td>\n",
              "      <td>[japan, suga, win, ruling, party, race, replace, premier, abe]</td>\n",
              "      <td>[japan suga, suga win, win ruling, ruling party, party race, race replace, replace premier, premier abe]</td>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16301 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                              title  ... topic_topic_label\n",
              "273937  Samsung eyes new opportunities as pandemic ignites demand for home appliances                ...  NaN             \n",
              "273938  Horse racing: Tiz the Law arrives, Art Collector exits at Churchill Downs                    ...  NaN             \n",
              "273939  US Open 2020: Osaka passes Doi test and Pliskova advances but Gauff falls                    ...  NaN             \n",
              "273940  The future of sports TV coverage? Eurosport unveils mixed-reality 'Cube' studio for US Open  ...  NaN             \n",
              "273941  U.S. employment projected to increase six million from 2019 to 2029: Labor Department        ...  NaN             \n",
              "...                                                                                       ...        ...  ...             \n",
              "290474  Trump signs new, expanded executive order to lower U.S. drug prices                          ...  NaN             \n",
              "290475  Nuggets force Game 7 and D'Antoni leaves HOU                                                 ...  NaN             \n",
              "290476  Health Official Out To Manipulate CDC Reports Has Deep Russian Ties                          ...  NaN             \n",
              "290477  Global Markets: Asian shares buoyed by coronavirus vaccine hopes                             ...  NaN             \n",
              "290478  Japan’s Suga Wins Ruling Party Race to Replace Premier Abe                                   ...  NaN             \n",
              "\n",
              "[16301 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWSpTb-KOsSV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "e74e3a2f-6338-4b4a-f23a-e651bca439a5"
      },
      "source": [
        "df_most_rep = df2\n",
        "\n",
        "#compare 2 lists of tokens for matching comparison, essentially, it tells us if the tokens of this news headlines are the keywords used for the topic\n",
        "df_most_rep['topic_topic_label'] = df_most_rep['topic_topic_label'].apply(lambda x: re.findall(r'\"(.*?)\"', x))\n",
        "\n",
        "shared_tokens = []\n",
        "for j in tqdm(range(len(df_most_rep))):\n",
        "    line = df_most_rep.iloc[j]\n",
        "    tokensA = line['topic_topic_label']\n",
        "    tokensB = line['ngram_token']\n",
        "    tokensC = list(set(tokensA).intersection(set(tokensB)))\n",
        "    shared_tokens.append(tokensC)\n",
        "\n",
        "df_most_rep['common tokens'] = pd.Series(shared_tokens)\n",
        "df_most_rep['common tokens count'] = df_most_rep['common tokens'].apply(lambda x: len(x))\n",
        "\n",
        "save_checkpoint(df_most_rep,'/content/gdrive/My Drive/group/data/topic_modelled_news_headlines.h5') ########调\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-210-36cd4cd5e61e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#compare 2 lists of tokens for matching comparison, essentially, it tells us if the tokens of this news headlines are the keywords used for the topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_most_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_topic_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_most_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_topic_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\"(.*?)\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mshared_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4198\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4200\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-210-36cd4cd5e61e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#compare 2 lists of tokens for matching comparison, essentially, it tells us if the tokens of this news headlines are the keywords used for the topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_most_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_topic_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_most_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_topic_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\"(.*?)\"'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mshared_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuzkdyxyYLFl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "060a2194-0341-4096-b25e-379a55777f50"
      },
      "source": [
        "#junk code used for finding the top n tweets\n",
        "'''\n",
        "for i in range(1,13):\n",
        "    print('Month '+ str(i))\n",
        "    df_most_rep = pd.read_pickle('gdrive/My Drive/IO - twitter datasets/russia/topicmodellingIRA_classified_{}.h5'.format(i))\n",
        "\n",
        "    df_most_rep = df_most_rep[df_most_rep['tweet_year'] == 2016]\n",
        "\n",
        "        \n",
        "    #find top 10 tweets for each category\n",
        "    df_most_rep = df_most_rep[df_most_rep['common tokens count']>0] #filter for shared words with topics\n",
        "    \n",
        "\n",
        "    sent_topics_outdf_grpd = df_most_rep[['tweet_text','user_screen_name', 'retweet_count','ngram_token','topic_pred_topic',\n",
        "                 'topic_max_pred_value', 'topic_topic_label','common tokens','common tokens count']].groupby('topic_pred_topic')\n",
        "\n",
        "    sent_topics_sorteddf = pd.DataFrame()\n",
        "    for k, grp in sent_topics_outdf_grpd:\n",
        "        sent_topics_sorteddf = pd.concat([sent_topics_sorteddf, \n",
        "                                                grp.sort_values(['retweet_count'], ascending=[0]).head(5)], \n",
        "                                                axis=0)\n",
        "\n",
        "    sent_topics_sorteddf.reset_index(drop=True, inplace=True)\n",
        "    sent_topics_sorteddf.to_excel('gdrive/My Drive/IO - twitter datasets/russia/top_tweets_IRA_2016_month{}.xlsx'.format(i))\n",
        "    sent_topics_sorteddf.to_pickle('gdrive/My Drive/IO - twitter datasets/russia/sortedtoptentopicmodellingIRA_classified_{}.h5'.format(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-181-62c049e360eb>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    sent_topics_sorteddf.to_pickle('gdrive/My Drive/IO - twitter datasets/russia/sortedtoptentopicmodellingIRA_classified_{}.h5'.format(i))\u001b[0m\n\u001b[0m                                                                                                                                           \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a_BPOHITjhs"
      },
      "source": [
        "# To do analysis and results reporting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUOTnv7UGo3n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "outputId": "b85f2875-e175-40a2-b97e-fe9f2a268e23"
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "df_topic_sents_keywords = pd.read_pickle('/content/gdrive/My Drive/group/data/topic_modelled_news_headlines.h5') ########调\n",
        "\n",
        "topic_num_keywords = df_topic_sents_keywords[['topic_pred_topic', 'topic_topic_label']]\n",
        "\n",
        "topic_num_keywords['topic_topic_label'] = topic_num_keywords['topic_topic_label'].apply(lambda x: str(x))\n",
        "topic_num_keywords = topic_num_keywords.groupby(['topic_pred_topic', 'topic_topic_label']).count().reset_index()    \n",
        "\n",
        "\n",
        "topic_counts = df_topic_sents_keywords['topic_pred_topic'].value_counts().reset_index()\n",
        "topic_counts.columns = ['topic_pred_topic','counts']\n",
        "topic_contribution = topic_counts['counts'].apply(lambda x: round(x/topic_counts.counts.sum(), 3)*100)\n",
        "dfa = pd.concat([topic_counts,topic_contribution],axis=1)\n",
        "\n",
        "\n",
        "df_dominant_topics = pd.merge(topic_num_keywords,dfa,on='topic_pred_topic')\n",
        "df_dominant_topics.columns = ['Dominant Topic', 'Topic Keywords', 'Number of Documents', 'Percentage of Tweets']\n",
        "\n",
        "df_dominant_topics['Dominant Topic'] = df_dominant_topics.index\n",
        "\n",
        "df_dominant_topics['Topic Keywords'] = df_dominant_topics['Topic Keywords'].apply(lambda x: ast.literal_eval(x))\n",
        "df_dominant_topics['Topic Keywords'] = df_dominant_topics['Topic Keywords'].apply(lambda x: ', '.join([str(elem) for elem in x]) )\n",
        "df_dominant_topics['Topic Keywords'] = df_dominant_topics['Dominant Topic'].apply(lambda x: 'Topic ' + str(x) + ': ') + df_dominant_topics['Topic Keywords']\n",
        "\n",
        "display(df_dominant_topics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dominant Topic</th>\n",
              "      <th>Topic Keywords</th>\n",
              "      <th>Number of Documents</th>\n",
              "      <th>Percentage of Tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Topic 0: coronavirus vaccine, coronavirus concern, due coronavirus, case coronavirus, bbc news, china travel, asia stock, scientific american, coronavirus test, treasury yield</td>\n",
              "      <td>86</td>\n",
              "      <td>4.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Topic 1: coronavirus fear, virus outbreak, novel coronavirus, china economy, new virus, deadly coronavirus, public health, health emergency, fall coronavirus, asian market</td>\n",
              "      <td>113</td>\n",
              "      <td>5.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Topic 2: co uk, express co, news express, world news, coronavirus spread, new coronavirus, coronavirus latest, uk news, deadly virus, science news</td>\n",
              "      <td>913</td>\n",
              "      <td>46.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Topic 3: wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price</td>\n",
              "      <td>127</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Topic 4: news guardian, world news, death toll, coronavirus death, football sport, sport express, transfer news, toll rise, european stock, spread cnn</td>\n",
              "      <td>175</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>Topic 5: china virus, virus fear, u stock, update oil, virus toll, china flight, declares coronavirus, fear grow, public health, emerge market</td>\n",
              "      <td>126</td>\n",
              "      <td>6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>Topic 6: coronavirus outbreak, amid coronavirus, coronavirus could, coronavirus case, say coronavirus, south china, global health, china morning, morning post, outbreak china</td>\n",
              "      <td>159</td>\n",
              "      <td>8.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>Topic 7: global market, travel express, news travel, coronavirus cnn, travel news, travel china, u case, emerge market, stock fall, co uk</td>\n",
              "      <td>82</td>\n",
              "      <td>4.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>Topic 8: china coronavirus, virus spread, coronavirus china, coronavirus u, new year, lunar new, wuhan virus, spread coronavirus, coronavirus outbreak, news guardian</td>\n",
              "      <td>98</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>Topic 9: stock market, hong kong, outbreak cnn, update marketscreener, live update, market open, coronavirus uk, coronavirus outbreak, coronavirus hit, co uk</td>\n",
              "      <td>74</td>\n",
              "      <td>3.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Dominant Topic  ... Percentage of Tweets\n",
              "0  0               ...  4.4                \n",
              "1  1               ...  5.8                \n",
              "2  2               ...  46.7               \n",
              "3  3               ...  6.5                \n",
              "4  4               ...  9.0                \n",
              "5  5               ...  6.5                \n",
              "6  6               ...  8.1                \n",
              "7  7               ...  4.2                \n",
              "8  8               ...  5.0                \n",
              "9  9               ...  3.8                \n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qTZ-imHG_5X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5130ec3-7f32-44d0-9f68-6c95ecbfb8cc"
      },
      "source": [
        "dis = pd.read_pickle('/content/gdrive/My Drive/group/data/topic_modelled_news_headlines.h5')              ########调\n",
        "topicN = 3                                                                                                                      ########调\n",
        "dis[dis['topic_pred_topic']==topicN]                                                                                             \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>topic_area</th>\n",
              "      <th>token</th>\n",
              "      <th>ngram_token</th>\n",
              "      <th>month</th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "      <th>topic_8</th>\n",
              "      <th>topic_9</th>\n",
              "      <th>topic_max_pred_value</th>\n",
              "      <th>topic_pred_topic</th>\n",
              "      <th>topic_topic_label</th>\n",
              "      <th>common tokens</th>\n",
              "      <th>common tokens count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tesla (TSLA) Breaks Shipment Record, Beats Estimates for Fourth Quarter Vehicles Shipped</td>\n",
              "      <td>2020-01-03</td>\n",
              "      <td>business</td>\n",
              "      <td>[tesla, tsla, break, shipment, record, beat, estimate, fourth, quarter, vehicle, ship]</td>\n",
              "      <td>[tesla tsla, tsla break, break shipment, shipment record, record beat, beat estimate, estimate fourth, fourth quarter, quarter vehicle, vehicle ship]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.046584</td>\n",
              "      <td>0.049803</td>\n",
              "      <td>0.056376</td>\n",
              "      <td>0.553383</td>\n",
              "      <td>0.050380</td>\n",
              "      <td>0.048434</td>\n",
              "      <td>0.052530</td>\n",
              "      <td>0.046979</td>\n",
              "      <td>0.049592</td>\n",
              "      <td>0.045942</td>\n",
              "      <td>0.553383</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[fourth quarter]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>Japan confirms first case of new China coronavirus strain | World news | The Guardian</td>\n",
              "      <td>2020-01-16</td>\n",
              "      <td>general</td>\n",
              "      <td>[japan, confirms, first, case, new, china, coronavirus, strain, world, news, guardian]</td>\n",
              "      <td>[japan confirms, confirms first, first case, case new, new china, china coronavirus, coronavirus strain, strain world, world news, news guardian]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.015442</td>\n",
              "      <td>0.016509</td>\n",
              "      <td>0.018693</td>\n",
              "      <td>0.351987</td>\n",
              "      <td>0.327934</td>\n",
              "      <td>0.016055</td>\n",
              "      <td>0.017414</td>\n",
              "      <td>0.015573</td>\n",
              "      <td>0.205164</td>\n",
              "      <td>0.015229</td>\n",
              "      <td>0.351987</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[first case, confirms first]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>New China virus: US announces first case - BBC News</td>\n",
              "      <td>2020-01-21</td>\n",
              "      <td>general</td>\n",
              "      <td>[new, china, virus, u, announces, first, case, bbc, news]</td>\n",
              "      <td>[new china, china virus, virus u, u announces, announces first, first case, case bbc, bbc news]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.272719</td>\n",
              "      <td>0.024797</td>\n",
              "      <td>0.028071</td>\n",
              "      <td>0.277426</td>\n",
              "      <td>0.025086</td>\n",
              "      <td>0.274786</td>\n",
              "      <td>0.026155</td>\n",
              "      <td>0.023391</td>\n",
              "      <td>0.024693</td>\n",
              "      <td>0.022875</td>\n",
              "      <td>0.277426</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[first case]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Dow tumbles on first case of coronavirus in the United States - CNN</td>\n",
              "      <td>2020-01-21</td>\n",
              "      <td>general</td>\n",
              "      <td>[dow, tumble, first, case, coronavirus, united, state, cnn]</td>\n",
              "      <td>[dow tumble, tumble first, first case, case coronavirus, coronavirus united, united state, state cnn]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.236655</td>\n",
              "      <td>0.033109</td>\n",
              "      <td>0.037479</td>\n",
              "      <td>0.497397</td>\n",
              "      <td>0.033493</td>\n",
              "      <td>0.032199</td>\n",
              "      <td>0.034922</td>\n",
              "      <td>0.031232</td>\n",
              "      <td>0.032969</td>\n",
              "      <td>0.030543</td>\n",
              "      <td>0.497397</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[first case]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>Wuhan coronavirus case confirmed in Washington state - CNN</td>\n",
              "      <td>2020-01-21</td>\n",
              "      <td>general</td>\n",
              "      <td>[wuhan, coronavirus, case, confirm, washington, state, cnn]</td>\n",
              "      <td>[wuhan coronavirus, coronavirus case, case confirm, confirm washington, washington state, state cnn]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.023195</td>\n",
              "      <td>0.024798</td>\n",
              "      <td>0.028072</td>\n",
              "      <td>0.777610</td>\n",
              "      <td>0.025085</td>\n",
              "      <td>0.024116</td>\n",
              "      <td>0.026166</td>\n",
              "      <td>0.023391</td>\n",
              "      <td>0.024692</td>\n",
              "      <td>0.022875</td>\n",
              "      <td>0.777610</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[wuhan coronavirus, case confirm]</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1739</th>\n",
              "      <td>Visa Inc (V) Q1 2020 Earnings Call Transcript | The Motley Fool</td>\n",
              "      <td>2020-01-31</td>\n",
              "      <td>business</td>\n",
              "      <td>[visa, inc, v, q, earnings, call, transcript, motley, fool]</td>\n",
              "      <td>[visa inc, inc v, v q, q earnings, earnings call, call transcript, transcript motley, motley fool]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.023195</td>\n",
              "      <td>0.024797</td>\n",
              "      <td>0.028070</td>\n",
              "      <td>0.777623</td>\n",
              "      <td>0.025085</td>\n",
              "      <td>0.024116</td>\n",
              "      <td>0.026155</td>\n",
              "      <td>0.023391</td>\n",
              "      <td>0.024692</td>\n",
              "      <td>0.022875</td>\n",
              "      <td>0.777623</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[motley fool, earnings call, q earnings]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1740</th>\n",
              "      <td>Illinois Tool Works Inc (ITW) Q4 2019 Earnings Call Transcript | The Motley Fool</td>\n",
              "      <td>2020-01-31</td>\n",
              "      <td>business</td>\n",
              "      <td>[illinois, tool, work, inc, itw, q, earnings, call, transcript, motley, fool]</td>\n",
              "      <td>[illinois tool, tool work, work inc, inc itw, itw q, q earnings, earnings call, call transcript, transcript motley, motley fool]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.023195</td>\n",
              "      <td>0.024797</td>\n",
              "      <td>0.028070</td>\n",
              "      <td>0.777623</td>\n",
              "      <td>0.025085</td>\n",
              "      <td>0.024116</td>\n",
              "      <td>0.026155</td>\n",
              "      <td>0.023391</td>\n",
              "      <td>0.024692</td>\n",
              "      <td>0.022875</td>\n",
              "      <td>0.777623</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[motley fool, earnings call, q earnings]</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1744</th>\n",
              "      <td>iQIYI Reschedules the Announcement Date of its Fourth Quarter and Fiscal Year 2019 Financial Results to February 27, 2020</td>\n",
              "      <td>2020-01-31</td>\n",
              "      <td>finance</td>\n",
              "      <td>[iqiyi, reschedules, announcement, date, fourth, quarter, fiscal, year, financial, result, february]</td>\n",
              "      <td>[iqiyi reschedules, reschedules announcement, announcement date, date fourth, fourth quarter, quarter fiscal, fiscal year, year financial, financial result, result february]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.046584</td>\n",
              "      <td>0.049803</td>\n",
              "      <td>0.056376</td>\n",
              "      <td>0.553383</td>\n",
              "      <td>0.050380</td>\n",
              "      <td>0.048434</td>\n",
              "      <td>0.052530</td>\n",
              "      <td>0.046979</td>\n",
              "      <td>0.049592</td>\n",
              "      <td>0.045942</td>\n",
              "      <td>0.553383</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[fourth quarter]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1820</th>\n",
              "      <td>'There's no doubt': Top US infectious disease doctor says Wuhan coronavirus can spread even when people have no symptoms  - CNN</td>\n",
              "      <td>2020-01-31</td>\n",
              "      <td>general</td>\n",
              "      <td>[doubt, top, u, infectious, disease, doctor, say, wuhan, coronavirus, spread, even, people, symptom, cnn]</td>\n",
              "      <td>[doubt top, top u, u infectious, infectious disease, disease doctor, doctor say, say wuhan, wuhan coronavirus, coronavirus spread, spread even, even people, people symptom, symptom cnn]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030970</td>\n",
              "      <td>0.033110</td>\n",
              "      <td>0.037520</td>\n",
              "      <td>0.703032</td>\n",
              "      <td>0.033497</td>\n",
              "      <td>0.032199</td>\n",
              "      <td>0.034926</td>\n",
              "      <td>0.031233</td>\n",
              "      <td>0.032970</td>\n",
              "      <td>0.030542</td>\n",
              "      <td>0.703032</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[wuhan coronavirus]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1922</th>\n",
              "      <td>U.S. confirms its first person-to-person coronavirus transmission - Reuters</td>\n",
              "      <td>2020-01-31</td>\n",
              "      <td>business</td>\n",
              "      <td>[u, confirms, first, person, person, coronavirus, transmission, reuters]</td>\n",
              "      <td>[u confirms, confirms first, first person, person person, person coronavirus, coronavirus transmission, transmission reuters]</td>\n",
              "      <td>1</td>\n",
              "      <td>0.030969</td>\n",
              "      <td>0.033109</td>\n",
              "      <td>0.037479</td>\n",
              "      <td>0.703084</td>\n",
              "      <td>0.033493</td>\n",
              "      <td>0.032199</td>\n",
              "      <td>0.034922</td>\n",
              "      <td>0.031233</td>\n",
              "      <td>0.032969</td>\n",
              "      <td>0.030542</td>\n",
              "      <td>0.703084</td>\n",
              "      <td>3</td>\n",
              "      <td>[wuhan coronavirus, coronavirus marketwatch, q earnings, fourth quarter, first case, case confirm, confirms first, motley fool, earnings call, oil price]</td>\n",
              "      <td>[confirms first]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>127 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                title  ... common tokens count\n",
              "1     Tesla (TSLA) Breaks Shipment Record, Beats Estimates for Fourth Quarter Vehicles Shipped                                         ...  1                 \n",
              "44    Japan confirms first case of new China coronavirus strain | World news | The Guardian                                            ...  2                 \n",
              "93    New China virus: US announces first case - BBC News                                                                              ...  1                 \n",
              "116   Dow tumbles on first case of coronavirus in the United States - CNN                                                              ...  1                 \n",
              "133   Wuhan coronavirus case confirmed in Washington state - CNN                                                                       ...  2                 \n",
              "...                                                          ...                                                                       ... ..                 \n",
              "1739  Visa Inc (V) Q1 2020 Earnings Call Transcript | The Motley Fool                                                                  ...  3                 \n",
              "1740  Illinois Tool Works Inc (ITW) Q4 2019 Earnings Call Transcript | The Motley Fool                                                 ...  3                 \n",
              "1744  iQIYI Reschedules the Announcement Date of its Fourth Quarter and Fiscal Year 2019 Financial Results to February 27, 2020        ...  1                 \n",
              "1820  'There's no doubt': Top US infectious disease doctor says Wuhan coronavirus can spread even when people have no symptoms  - CNN  ...  1                 \n",
              "1922  U.S. confirms its first person-to-person coronavirus transmission - Reuters                                                      ...  1                 \n",
              "\n",
              "[127 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbEQ3VqrVIe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "b778bbe1-1e57-48ac-ca99-169a565cf14b"
      },
      "source": [
        "#sort by predicted value\n",
        "topicN = 16\n",
        "dis = pd.read_pickle('/content/gdrive/My Drive/group/data/topic_modelled_news_headlines.h5')   ########调\n",
        "dis[dis['topic_pred_topic']==topicN].sort_values('topic_max_pred_value')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>topic_area</th>\n",
              "      <th>token</th>\n",
              "      <th>ngram_token</th>\n",
              "      <th>month</th>\n",
              "      <th>topic_0</th>\n",
              "      <th>topic_1</th>\n",
              "      <th>topic_2</th>\n",
              "      <th>topic_3</th>\n",
              "      <th>topic_4</th>\n",
              "      <th>topic_5</th>\n",
              "      <th>topic_6</th>\n",
              "      <th>topic_7</th>\n",
              "      <th>topic_8</th>\n",
              "      <th>topic_9</th>\n",
              "      <th>topic_max_pred_value</th>\n",
              "      <th>topic_pred_topic</th>\n",
              "      <th>topic_topic_label</th>\n",
              "      <th>common tokens</th>\n",
              "      <th>common tokens count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [title, date, topic_area, token, ngram_token, month, topic_0, topic_1, topic_2, topic_3, topic_4, topic_5, topic_6, topic_7, topic_8, topic_9, topic_max_pred_value, topic_pred_topic, topic_topic_label, common tokens, common tokens count]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mANGq_Do22Y5"
      },
      "source": [
        "#export\n",
        "dis.to_csv('/content/gdrive/My Drive/group/data/topics.csv',encoding='utf-8') ########调"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cGuAVwfEXj5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}